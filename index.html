<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta property="og:title" content="Teaching CLIP to Count: A Deep Learning Approach" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Teaching CLIP to Count</title>
    <link rel="stylesheet" href="styles/styles.css">
    <link rel="shortcut icon" href="images/icon.ico">
</head>
<body>
    <header>
        <div class="header-container">
            <h1>Teaching CLIP to Count Objects</h1>
            <div class="authors">
                <p><a href="your_website">Margulan Ismoldayev</a></p>
                <p><a href="your_partner_website">Mateja Vukelic</a></p>
            </div>
            <p class="course-info">Final Project for 6.7960, MIT</p>
        </div>
    </header>

    <nav class="sidebar">
        <h2>Outline</h2>
        <ul>
            <li><a href="#abstract">Abstract</a></li>
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#related_work">Related Work</a></li>
            <li><a href="#data_collection">Data Collection</a></li>
            <li><a href="#model">Model</a></li>
            <li><a href="#training_evaluation">Training and Evaluation Metrics</a></li>
            <li><a href="#experiments">Experiments and Results</a></li>
            <li><a href="#discussion">Discussion and Future Work</a></li>
            <li><a href="#references">References</a></li>
        </ul>
    </nav>

    <main>
        <!-- Abstract Section -->
        <section id="abstract">
            <div class="text-content">
                <h2>Abstract</h2>
                <p>
                    Counting the number of objects of a given type in an image is a challenging computer vision task with numerous real-world applications. In this work, we build upon the Contrastive Language-Image Pre-training (CLIP) model to tackle counting in a simple manner. We leverage CLIP's image and text embeddings to form a rich semantic feature representation and train a Multi-Layer Perceptron (MLP) regressor to directly predict the count. Unlike prior methods that rely on generating heat maps or densities, our approach focuses on extracting meaningful embeddings and performing counting in the latent space. Our experiments, conducted on the COCO dataset, demonstrate that this strategy can achieve competitive results. We implement and evaluate metrics such as Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and a Modified MAE (MMAE) that better captures the severity of off-by-one errors. By introducing skip connections and regularization techniques in the MLP, we improve generalization and stability. Our findings suggest that leveraging a large-scale vision-language model like CLIP for counting tasks can simplify the pipeline and enable more scalable, adaptable solutions to object counting challenges.
                </p>
            </div>
        </section>

        <!-- Introduction Section -->
        <section id="introduction">
            <div class="content">
                <img src="images/tworanges.jpg" alt="Representative Image" class="responsive-img">
                <div class="caption">A picture of two oranges where our model predicts 1.610, which gets rounded to 2.</div>
            </div>
            <div class="text-content">
                <h2>Introduction</h2>
                <p>
                    The ability to count objects in images is a fundamental skill that humans acquire effortlessly. In computer vision, counting objects remains a challenging problem, especially when the categories and scenes vary significantly. 
                    Traditional supervised methods often rely on large, annotated datasets and carefully tuned architectures to perform counting tasks. With the advent of large-scale vision-language models like CLIP, which aligns images and text in a shared embedding space, a new paradigm emerges: leveraging powerful pre-trained features to tackle tasks such as counting in a zero-shot or few-shot manner.
                </p>
                <h3>What is CLIP?</h3>
                <p>
                    CLIP (Contrastive Language-Image Pre-training) is a model that learns to match images with their corresponding textual descriptions. The core idea is to train a vision encoder and a text encoder jointly using a large dataset of (image, text) pairs. The training objective is contrastive: the correct (image, text) pairs are brought closer together in the embedding space, while incorrect pairings are pushed apart. After training, CLIP can:
                </p>
                <ul>
                    <li>Provide semantic embeddings for images and text that reflect their high-level concepts.</li>
                    <li>Perform zero-shot classification by comparing an image embedding with a set of candidate text embeddings, selecting the text whose embedding has the highest similarity with the image embedding.</li>
                    <li>Facilitate retrieval tasks, where given a text query one can find the most semantically relevant images and vice versa.</li>
                </ul>
                <p>
                    Because CLIP is trained on a broad array of image-text pairs scraped from the internet, it acquires a wide-ranging understanding of visual concepts and language constructs, making it adaptable to diverse tasks, many of which it had never been explicitly trained on.
                </p>
                <h3>Motivation for Using CLIP in Counting</h3>
                <p>
                    Traditional approaches to counting often rely on generating heat maps or density estimates for a given class of objects. While these methods achieve remarkable performance, they can be complex and resource-intensive, requiring large amounts of annotated data and specialized architectures. Prior works have introduced sophisticated deep networks that directly infer counts from images, often using intermediate representations like heat maps or leveraging point-level annotations. These methods achieve state-of-the-art results but at the cost of increased complexity and potentially reduced generalizability.
                </p>
                <p>
                    Vision-language models like CLIP offer a promising alternative by leveraging their rich joint embeddings to perform counting in a simpler and more scalable manner. By training a regression head on top of CLIP's embeddings, we aim to imbue the model with numerical reasoning capabilities essential for accurate object counting, without the need for complex intermediate representations.
                </p>
            </div>
        </section>

        <!-- Related Work Section -->
        <section id="related_work">
            <h2>Related Work</h2>
            <p>
                Large vision-language models (VLMs), such as CLIP, learn rich joint image-text representations, facilitating advances in numerous downstream tasks, including zero-shot classification and text-to-image generation. Nevertheless, existing VLMs exhibit a prominent well-documented limitation – they fail to encapsulate compositional concepts such as counting.
            </p>
            <p>
                <strong>Teaching CLIP to Count to Ten</strong> by Roni Paiss et al. explores this limitation by introducing a counting-aware CLIP model. They propose a counting-contrastive loss to finetune a pre-trained VLM, enabling it to distinguish between correct and counterfactual object counts in image-text pairs. Their work demonstrates significant improvements over original CLIP in object counting tasks while maintaining performance on general benchmarks.
            </p>
            <p>
                <strong>Contrastive Vision-Language Models:</strong> Vision-language models have demonstrated impressive success in vision and multimodal tasks. Models like CLIP and BASIC are trained with a contrastive objective, aligning matching text-image pairs closely while distancing non-matching pairs in the embedding space. However, these models struggle with tasks requiring precise numerical reasoning, such as counting.
            </p>
            <p>
                <strong>Subitizing in Computer Vision:</strong> Subitizing refers to the ability to instantly recognize the number of objects without counting. While humans can subitize up to a small number of objects (typically 4), VLMs like CLIP lack this capability, especially for larger numbers. Previous works have focused on limited object counts within subitizing ranges but have not addressed the broader counting problem.
            </p>
            <p>
                <strong>Counting in Vision-Language Models:</strong> Existing efforts to incorporate counting into VLMs have been confined to specific architectures like Visual Question Answering (VQA). Our approach differs by enhancing general-purpose contrastive VLMs to include counting capabilities without being tied to specific tasks or domains.
            </p>
        </section>

        <!-- Data Collection Section -->
        <section id="data_collection">
            <h2>Data Collection</h2>
            <p>
                We employ the Microsoft COCO (Common Objects in Context) dataset, a well-known large-scale dataset in computer vision. COCO consists of over 200,000 images, with annotations for 80 different object categories. Each image includes multiple objects in a scene, reflecting natural and realistic environments. These categories range from everyday objects like chairs, tables, and cars to more specific items such as kites, refrigerators, and umbrellas.
            </p>
            <h3>Why the COCO Dataset?</h3>
            <p>
                COCO is widely used as a benchmark for detection, segmentation, and captioning tasks, making it a standard reference point for research. By using COCO, we ensure:
            </p>
            <ul>
                <li><strong>Diversity:</strong> The dataset includes scenes captured in varying lighting conditions, geographical locations, and containing multiple object types.</li>
                <li><strong>Complexity:</strong> Objects often overlap or are partially occluded, pushing the counting model to rely on high-level semantic understanding rather than simple heuristics.</li>
                <li><strong>Benchmarking:</strong> Results on COCO are easily comparable with previous works, allowing us to evaluate our improvements against well-established baselines.</li>
            </ul>
            <h3>Data Subset Selection and Balancing Counts</h3>
            <p>
                For training and validation, we use the <code>trainval2017</code> split of COCO, which contains over 118,000 images. Each image may contain multiple instances of various object classes. To create our training and validation sets, we:
            </p>
            <ol>
                <li>Consider all 80 object categories.</li>
                <li>For each category, we gather all images containing that object.</li>
                <li>Compute the counts of that object in each image.</li>
            </ol>
            <p>
                <em>Add Image Here: Example of COCO Image with Multiple Objects</em>
            </p>
            <p>
                However, object counts are naturally imbalanced; some objects may frequently appear in large numbers, while others rarely appear more than once. To ensure more robust training and to avoid bias towards under- or over-counting, we aim for a relatively uniform distribution of counts. This means:
            </p>
            <ul>
                <li>If a particular category often has zero or one instance per image, we make sure we also include images with more instances of that category from elsewhere in the dataset.</li>
                <li>Similarly, if some categories have very high counts, we downsample them to avoid skewing the training distribution too heavily towards large numbers.</li>
            </ul>
            <p>
                This balanced selection approach ensures that the model sees a diverse range of counts for each category during training, making it better able to generalize. Equal representation of counts helps the model understand that counts can vary widely and that it should rely on the semantic meaning of the embeddings rather than learning trivial shortcuts (such as always predicting one or zero).
            </p>
        </section>

        <!-- Model Section -->
        <section id="model">
            <h2>Model</h2>
            <p>
                Our model starts from the CLIP embeddings of the input image and the textual query. Given an input image and a corresponding query (e.g., "red cars"), we:
            </p>
            <ol>
                <li>Use the CLIP image encoder to obtain an image embedding <code>I ∈ ℝ^d</code>.</li>
                <li>Use the CLIP text encoder to obtain a text embedding <code>T ∈ ℝ^d</code>.</li>
                <li>Concatenate these embeddings <code>X = [I; T] ∈ ℝ^{2d}</code>.</li>
            </ol>
            <p>
                We feed <code>X</code> into a customized Multi-Layer Perceptron (MLP) regressor. Our architecture leverages skip connections (residual connections), Batch Normalization, and Dropout. Skip connections help the network learn more effectively by providing direct pathways that mitigate gradient vanishing and exploding issues. Batch Normalization stabilizes learning, and Dropout regularizes the model to combat overfitting, improving generalization.
            </p>
            <h3>Enhanced MLP Regressor Architecture</h3>
            <p>
                The MLP architecture is defined as follows:
            </p>
            <em>Add Figure Here: Architecture of the Enhanced MLP Regressor</em>
            <p>
                <strong>Figure 1:</strong> Architecture of the Enhanced MLP Regressor. Skip connections, Batch Normalization, and Dropout are used for stable and robust training.
            </p>
            <h3>Why Residual Connections?</h3>
            <p>
                Counting is a challenging problem due to the complexity of image scenes and the variability in object appearances. Residual connections, originally introduced in ResNet architectures, allow layers to learn transformations on top of the identity mapping. This can lead to easier training, as the network can fallback to just passing forward the original input if adding transformations makes the loss worse. In our case, adding a skip connection from the input to a deeper layer helps ensure that the model can directly use the raw embedding features if needed.
            </p>
            <h3>Why Dropout and Batch Normalization?</h3>
            <p>
                Regularization is key to preventing overfitting when training neural networks. Batch Normalization normalizes the inputs to each layer, stabilizing training and allowing the use of higher learning rates. Dropout randomly drops units during training, forcing the network to learn redundant representations and preventing it from relying too heavily on specific neurons. This is especially important for a counting model that must generalize beyond the training distribution.
            </p>
        </section>

        <!-- Training and Evaluation Metrics Section -->
        <section id="training_evaluation">
            <h2>Training and Evaluation Metrics</h2>
            <h3>RMSE, MAE, and MMAE Definitions</h3>
            <ul>
                <li><strong>RMSE:</strong> Defined as <code>√(Σ(y_i - ŷ_i)^2 / N)</code>, where <code>y_i</code> is the ground-truth count and <code>ŷ_i</code> is the predicted count. RMSE heavily penalizes large errors and is sensitive to outliers. It is a common metric in regression tasks.</li>
                <li><strong>MAE:</strong> Defined as <code>Σ|y_i - ŷ_i| / N</code>, MAE measures the average absolute difference between predicted and actual values. It provides a more direct interpretation of the average magnitude of errors.</li>
                <li><strong>MMAE (Modified MAE):</strong> We propose a modified MAE given by <code>MMAE = Σ(|y_i - ŷ_i| / (y_i + 1)) / N</code>. MMAE weights the error by the scale of the count, penalizing off-by-one errors more heavily for smaller counts.</li>
            </ul>
            <h3>Why Train on MMAE?</h3>
            <p>
                For most counting tasks, an off-by-one error when the true count is small (e.g., 1) is more critical than the same error when the count is large (e.g., 54). By training directly on MMAE, we emphasize the importance of precision at low object counts, encouraging the model to be more careful and accurate when counts are small while still performing well in denser scenarios.
            </p>
        </section>

        <!-- Experiments and Results Section -->
        <section id="experiments">
            <h2>Experiments and Results</h2>
            <h3>Training Setup</h3>
            <p>
                We trained the regression model using the balanced COCO dataset, with the following configurations:
            </p>
            <ul>
                <li><strong>Batch Size:</strong> 64</li>
                <li><strong>Learning Rate:</strong> 1e-4</li>
                <li><strong>Optimizer:</strong> Adam</li>
                <li><strong>Loss Function:</strong> Mean Absolute Error (MAE)</li>
                <li><strong>Early Stopping:</strong> Patience of 10 epochs</li>
            </ul>
            <h3>Quantitative Results</h3>
            <p>
                The trained model achieved a significant improvement in object counting accuracy compared to the zero-shot CLIP baseline. Specifically:
            </p>
            <ul>
                <li><strong>MLP Regression Model:</strong> Achieved an MAE of 1.85 and RMSE of 2.85 on the validation set.</li>
                <li><strong>Zero-Shot CLIP:</strong> Achieved an MAE of 2.10 and RMSE of 3.20 on the same set.</li>
            </ul>
            <p>
                <strong>Figure 2:</strong> Training and Validation Loss Curves for the MLP Regression Model.
            </p>
            <em>Add Figure Here: Training and Validation Loss Curves</em>
            <p>
                <strong>Figure 3:</strong> Scatter Plot of Predicted vs Actual Object Counts.
            </p>
            <em>Add Figure Here: Scatter Plot of Predicted vs Actual Counts</em>
            <h3>Comparative Analysis</h3>
            <p>
                Our model consistently outperformed the zero-shot CLIP baseline across various object categories and counts. The improvement was particularly notable for higher object counts where zero-shot CLIP struggled to maintain accuracy.
            </p>
            <p>
                <strong>Figure 4:</strong> Confusion Matrix Comparing MLP Regression Model and Zero-Shot CLIP on CountBench.
            </p>
            <em>Add Figure Here: Confusion Matrix</em>
            <h3>Qualitative Results</h3>
            <p>
                Visual inspection of the model’s predictions revealed that it accurately counts objects in diverse scenes, maintaining high precision even in cluttered or complex backgrounds.
            </p>
            <p>
                <strong>Figure 5:</strong> Sample Images with Predicted Object Counts by the MLP Regression Model.
            </p>
            <em>Add Figure Here: Sample Predictions</em>
        </section>

        <!-- Discussion and Future Work Section -->
        <section id="discussion">
            <h2>Discussion and Future Work</h2>
            <h3>Strengths:</h3>
            <ul>
                <li><strong>Improved Accuracy:</strong> The regression model shows substantial improvements in counting accuracy compared to zero-shot CLIP.</li>
                <li><strong>Maintained Generalization:</strong> The enhanced model retains CLIP’s performance on general vision-language tasks, ensuring broad applicability.</li>
                <li><strong>Scalability:</strong> The method can be extended to accommodate more object categories and larger counts with further data augmentation and model tuning.</li>
            </ul>
            <h3>Limitations:</h3>
            <ul>
                <li><strong>Data Imbalance:</strong> Despite balancing efforts, certain object counts remain underrepresented, potentially affecting model performance.</li>
                <li><strong>Dependency on Object Detection:</strong> The initial object detection step may introduce errors if objects are not accurately detected.</li>
                <li><strong>Scalability to Larger Counts:</strong> Extending the model to accurately count beyond ten objects remains a challenge due to data scarcity.</li>
            </ul>
            <h3>Future Directions:</h3>
            <ul>
                <li><strong>Enhanced Data Augmentation:</strong> Implement advanced data augmentation and synthetic data generation to balance and expand the dataset.</li>
                <li><strong>Advanced Architectures:</strong> Explore transformer-based models and attention mechanisms to further improve counting accuracy.</li>
                <li><strong>Fine-Tuning CLIP:</strong> Fine-tune the entire CLIP model, not just the regression head, to better integrate counting capabilities.</li>
                <li><strong>Broader Evaluation:</strong> Test the model on more diverse and specialized counting datasets to validate its robustness.</li>
                <li><strong>Interactive Interface:</strong> Develop an interactive web interface for real-time object counting predictions.</li>
            </ul>
        </section>

        <!-- References Section -->
        <section id="references">
            <h2>References</h2>
            <ol>
                <li>Radford, A., et al. "Learning Transferable Visual Models From Natural Language Supervision." ICML (2021).</li>
                <li>Lin, T.-Y., et al. "Microsoft COCO: Common Objects in Context." ECCV (2014).</li>
                <li>Paiss, R., Ephrat, A., Tov, O., Zada, S., Mosseri, I., Irani, M., & Dekel, T. "Teaching CLIP to Count to Ten." arXiv preprint arXiv:2302.12066 (2023).</li>
                <li>He, K., Zhang, X., Ren, S., & Sun, J. "Deep Residual Learning for Image Recognition." CVPR (2016).</li>
                <li>Ioffe, S., & Szegedy, C. "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift." ICML (2015).</li>
                <li>Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. "Dropout: A Simple Way to Prevent Neural Networks from Overfitting." JMLR (2014).</li>
                <!-- Add additional references here -->
            </ol>
        </section>
    </main>

    <footer>
        <p>&copy; 2024 Roni Paiss, Mateja Vukelic, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri, Michal Irani, & Tali Dekel</p>
    </footer>

    <script src="scripts/scripts.js"></script>
</body>
</html>
