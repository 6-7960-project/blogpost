<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta property="og:title" content="Teaching CLIP to Count: A Deep Learning Approach" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Teaching CLIP to Count</title>
    <link rel="stylesheet" href="styles/styles.css">
    <link rel="shortcut icon" href="images/icon.ico">
</head>
<body>
    <header>
        <div class="header-container">
            <h1>Teaching CLIP to Count Objects</h1>
            <div class="authors">
                <p><a href="your_website">Your Name</a></p>
                <p><a href="your_partner_website">Collaborator Name</a></p>
            </div>
            <p class="course-info">Final Project for 6.7960, MIT</p>
        </div>
    </header>

    <nav class="sidebar">
        <h2>Outline</h2>
        <ul>
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#methodology">Methodology</a></li>
            <li><a href="#experiments">Experiments and Results</a></li>
            <li><a href="#discussion">Discussion and Future Work</a></li>
            <li><a href="#references">References</a></li>
        </ul>
    </nav>

    <main>
        <section id="intro">
            <div class="content">
                <img src="images/your_image_here.png" alt="Representative Image" class="responsive-img">
                <div class="caption">A representative image showcasing object counting scenario.</div>
            </div>
        </section>

        <section id="introduction">
            <h2>Introduction</h2>
            <p>
                The ability to count objects in images is a fundamental skill that humans acquire effortlessly. In computer vision, counting objects remains a challenging problem, especially when the categories and scenes vary significantly. Traditional supervised methods often rely on large, annotated datasets and carefully tuned architectures to perform counting tasks. With the advent of large-scale vision-language models like CLIP, which aligns images and text in a shared embedding space, a new paradigm emerges: leveraging powerful pre-trained features to tackle tasks such as counting in a zero-shot or few-shot manner.
            </p>
            <p>
                In this project, we build upon OpenAI's CLIP model to perform object counting. We aim to answer the question: can we train a simple regression head on top of CLIP’s joint image-text embeddings to accurately count objects of a given category, and how does this approach compare to CLIP’s inherent zero-shot capabilities?
            </p>
            <h3>Key Contributions:</h3>
            <ul>
                <li>Utilize CLIP’s pre-trained visual and textual encoders to extract embeddings for images and corresponding object class descriptions.</li>
                <li>Train a neural network regressor to predict the count of the specified object from the combined image-text embedding.</li>
                <li>Compare this learned model against a zero-shot CLIP baseline, where we select the most similar text prompt among multiple possible counts.</li>
            </ul>
            <h3>Motivation:</h3>
            <p>
                While CLIP excels at recognizing concepts and can sometimes estimate the presence or absence of objects, counting requires more granular numeric reasoning. By training a custom head on top of CLIP embeddings, we investigate whether CLIP’s strong representation space can be extended to handle counting tasks robustly.
            </p>
        </section>

        <section id="methodology">
            <h2>Methodology</h2>
            <p>
                <!-- Detailed methodology will be added here -->
            </p>
        </section>

        <section id="experiments">
            <h2>Experiments and Results</h2>
            <p>
                <!-- Detailed experiments and results will be added here -->
            </p>
        </section>

        <section id="discussion">
            <h2>Discussion and Future Work</h2>
            <p>
                <!-- Detailed discussion and future work will be added here -->
            </p>
        </section>

        <section id="references">
            <h2>References</h2>
            <ol>
                <li>Radford, A., et al. "Learning Transferable Visual Models From Natural Language Supervision." ICML (2021).</li>
                <li>Lin, T.-Y., et al. "Microsoft COCO: Common Objects in Context." ECCV (2014).</li>
                <!-- Additional references can be added here -->
            </ol>
        </section>
    </main>

    <footer>
        <p>&copy; 2024 Your Name &amp; Collaborator Name</p>
    </footer>
</body>
</html>
